{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHjsBYPRh6Yr7rlS5SlXIG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSd6GYbplGWq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nltk and the brown module\n",
        "import nltk\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "MD4iTZAylO95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the ncessary data from the dataset\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "id": "xA5FgZIelVL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in our corpus using the universal tag-set. The dataset is in the form of a ist of lists of tuples\n",
        "corpus = brown.tagged_sents(tagsets='universal')\n",
        "corpus"
      ],
      "metadata": {
        "id": "uGfFYIVMldXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the inputs and targets to make the dataset usable to hugingface\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for sentence_tag_pairs in corpus:\n",
        "  tokens = []\n",
        "  target = []\n",
        "  for token, tag in sentence_tag_pairs:\n",
        "    tokens.append(toekn)\n",
        "    target.append(tag)\n",
        "  inputs.append(tokens)\n",
        "  targets.append(target)"
      ],
      "metadata": {
        "id": "nefk4ZcrllGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save data to json format\n",
        "import json\n",
        "\n",
        "with open('data.jason', 'w') as f:\n",
        "  for x, y in zip(inputs,targets):\n",
        "    j = {'inputs':x, 'targets': y}\n",
        "    s = json.dumps(j)\n",
        "    f.write(f\"{s}\\n\")\n"
      ],
      "metadata": {
        "id": "lAMOtcahowfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import function\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "hsJuhMexpRuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call load_dataset\n",
        "data = load_dataset(\"json\", data_files='data.json')"
      ],
      "metadata": {
        "id": "vSnc0VlBpVif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the data\n",
        "data"
      ],
      "metadata": {
        "id": "Z_h6EUhOpePo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 60k samples is to much, shuffle the dataset and take the first 20k samples\n",
        "small = data[\"train\"].shuffle(seed=42).select(range(20_00))\n",
        "small"
      ],
      "metadata": {
        "id": "iLFkSVuHrk_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do a train test split\n",
        "data = small.train_test_split(seed=42)"
      ],
      "metadata": {
        "id": "z2Uu_mDlqxhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the first sample of the dataset, the input is a list of words, the target is a list of tags\n",
        "data[\"train\"][0]"
      ],
      "metadata": {
        "id": "XuYuHTV6q2dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the features attribute of our dataset\n",
        "data[\"train\"].features"
      ],
      "metadata": {
        "id": "A6MLnq_er8Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map targets to ints\n",
        "target_set = set()\n",
        "for target in targets:\n",
        "  target_set = target_set.union(target)\n",
        "target_set\n"
      ],
      "metadata": {
        "id": "lFmi71ILsAff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create id2label and label2id\n",
        "target_list = list(target_set)\n",
        "id2label = {k: v for k, v in enumerate(target_list)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ],
      "metadata": {
        "id": "7kc16dqasRCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the auto-tokenizer, you can try to use bert if u wish and compare the results\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Also try using Bert\n",
        "checkpoint = 'distilbert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "tEIj7Oo1si7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the tokenizer on the first sample of our dataset\n",
        "idx = 0\n",
        "t = tokenizer(data[\"train\"][idx][\"inputs\"], is_split_into_words=True)\n",
        "t"
      ],
      "metadata": {
        "id": "TnNBvNvCtpcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The output is not a dict\n",
        "type(t)"
      ],
      "metadata": {
        "id": "AFUm2ReRt41X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the batchencoding object has a tokens method\n",
        "t.tokens()"
      ],
      "metadata": {
        "id": "VDy0FnkVt7WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value of i indicates it is the i'th word\n",
        "# In the input sentence (counting from 0)\n",
        "t.word_ids()"
      ],
      "metadata": {
        "id": "m7P4u8IpuQQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "def align_targets(labels, word_ids):\n",
        "  aligned_labels = []\n",
        "  for word in word_ids:\n",
        "    if word is None:\n",
        "      # It's a token like [CLS]\n",
        "      label = -100\n",
        "    else:\n",
        "      # It's a real word\n",
        "      label = label2id[labels[word]]\n",
        "\n",
        "    # Add the label\n",
        "    aligned_labels.append(label)\n",
        "\n",
        "  return aligned_labels"
      ],
      "metadata": {
        "id": "JCUYGItT0DQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try our function\n",
        "labels = data['train'][idx]['targets']\n"
      ],
      "metadata": {
        "id": "nJWGKyyv0u4n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}