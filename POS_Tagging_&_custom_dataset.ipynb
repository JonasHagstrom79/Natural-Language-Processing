{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxzGrxV+Sq9B+XM6YA05Sg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonasHagstrom79/Natural-Language-Processing/blob/main/POS_Tagging_%26_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSd6GYbplGWq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nltk and the brown module\n",
        "import nltk\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "MD4iTZAylO95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the ncessary data from the dataset\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "id": "xA5FgZIelVL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in our corpus using the universal tag-set. The dataset is in the form of a ist of lists of tuples\n",
        "corpus = brown.tagged_sents(tagsets='universal')\n",
        "corpus"
      ],
      "metadata": {
        "id": "uGfFYIVMldXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the inputs and targets to make the dataset usable to hugingface\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for sentence_tag_pairs in corpus:\n",
        "  tokens = []\n",
        "  target = []\n",
        "  for token, tag in sentence_tag_pairs:\n",
        "    tokens.append(toekn)\n",
        "    target.append(tag)\n",
        "  inputs.append(tokens)\n",
        "  targets.append(target)"
      ],
      "metadata": {
        "id": "nefk4ZcrllGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save data to json format\n",
        "import json\n",
        "\n",
        "with open('data.jason', 'w') as f:\n",
        "  for x, y in zip(inputs,targets):\n",
        "    j = {'inputs':x, 'targets': y}\n",
        "    s = json.dumps(j)\n",
        "    f.write(f\"{s}\\n\")\n"
      ],
      "metadata": {
        "id": "lAMOtcahowfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import function\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "hsJuhMexpRuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call load_dataset\n",
        "data = load_dataset(\"json\", data_files='data.json')"
      ],
      "metadata": {
        "id": "vSnc0VlBpVif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the data\n",
        "data"
      ],
      "metadata": {
        "id": "Z_h6EUhOpePo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}