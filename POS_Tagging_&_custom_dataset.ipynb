{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK6OTjWU50Xu5im7ix/15j"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSd6GYbplGWq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nltk and the brown module\n",
        "import nltk\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "MD4iTZAylO95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the ncessary data from the dataset\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "id": "xA5FgZIelVL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in our corpus using the universal tag-set. The dataset is in the form of a ist of lists of tuples\n",
        "corpus = brown.tagged_sents(tagsets='universal')\n",
        "corpus"
      ],
      "metadata": {
        "id": "uGfFYIVMldXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the inputs and targets to make the dataset usable to hugingface\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for sentence_tag_pairs in corpus:\n",
        "  tokens = []\n",
        "  target = []\n",
        "  for token, tag in sentence_tag_pairs:\n",
        "    tokens.append(toekn)\n",
        "    target.append(tag)\n",
        "  inputs.append(tokens)\n",
        "  targets.append(target)"
      ],
      "metadata": {
        "id": "nefk4ZcrllGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save data to json format\n",
        "import json\n",
        "\n",
        "with open('data.jason', 'w') as f:\n",
        "  for x, y in zip(inputs,targets):\n",
        "    j = {'inputs':x, 'targets': y}\n",
        "    s = json.dumps(j)\n",
        "    f.write(f\"{s}\\n\")\n"
      ],
      "metadata": {
        "id": "lAMOtcahowfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import function\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "hsJuhMexpRuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call load_dataset\n",
        "data = load_dataset(\"json\", data_files='data.json')"
      ],
      "metadata": {
        "id": "vSnc0VlBpVif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the data\n",
        "data"
      ],
      "metadata": {
        "id": "Z_h6EUhOpePo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 60k samples is to much, shuffle the dataset and take the first 20k samples\n",
        "small = data[\"train\"].shuffle(seed=42).select(range(20_00))\n",
        "small"
      ],
      "metadata": {
        "id": "iLFkSVuHrk_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do a train test split\n",
        "data = small.train_test_split(seed=42)"
      ],
      "metadata": {
        "id": "z2Uu_mDlqxhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the first sample of the dataset, the input is a list of words, the target is a list of tags\n",
        "data[\"train\"][0]"
      ],
      "metadata": {
        "id": "XuYuHTV6q2dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the features attribute of our dataset\n",
        "data[\"train\"].features"
      ],
      "metadata": {
        "id": "A6MLnq_er8Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map targets to ints\n",
        "target_set = set()\n",
        "for target in targets:\n",
        "  target_set = target_set.union(target)\n",
        "target_set\n"
      ],
      "metadata": {
        "id": "lFmi71ILsAff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create id2label and label2id\n",
        "target_list = list(target_set)\n",
        "id2label = {k: v for k, v in enumerate(target_list)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ],
      "metadata": {
        "id": "7kc16dqasRCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the auto-tokenizer, you can try to use bert if u wish and compare the results\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Also try using Bert\n",
        "checkpoint = 'distilbert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "tEIj7Oo1si7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the tokenizer on the first sample of our dataset\n",
        "idx = 0\n",
        "t = tokenizer(data[\"train\"][idx][\"inputs\"], is_split_into_words=True)\n",
        "t"
      ],
      "metadata": {
        "id": "TnNBvNvCtpcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The output is not a dict\n",
        "type(t)"
      ],
      "metadata": {
        "id": "AFUm2ReRt41X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the batchencoding object has a tokens method\n",
        "t.tokens()"
      ],
      "metadata": {
        "id": "VDy0FnkVt7WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value of i indicates it is the i'th word\n",
        "# In the input sentence (counting from 0)\n",
        "t.word_ids()"
      ],
      "metadata": {
        "id": "m7P4u8IpuQQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define method, tak in list of labels and word id's, transforming frpm string to int\n",
        "def align_targets(labels, word_ids):\n",
        "  aligned_labels = []\n",
        "  for word in word_ids:\n",
        "    if word is None:\n",
        "      # It's a token like [CLS]\n",
        "      label = -100\n",
        "    else:\n",
        "      # It's a real word\n",
        "      label = label2id[labels[word]]\n",
        "\n",
        "    # Add the label\n",
        "    aligned_labels.append(label)\n",
        "\n",
        "  return aligned_labels"
      ],
      "metadata": {
        "id": "JCUYGItT0DQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try our function\n",
        "labels = data['train'][idx]['targets']\n",
        "word_ids = t.word_ids()\n",
        "aligned_targets = align_targets(labels, word_ids)\n",
        "aligned_targets"
      ],
      "metadata": {
        "id": "nJWGKyyv0u4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the aligned labels with the tokenized inputs\n",
        "aligned_labels = [id2label[i] if i >= 0 else None for i in aligned_targets]\n",
        "for x, y in zip(t.tokens(), aligned_labels):\n",
        "  print(f\"{x}\\t{y}\")"
      ],
      "metadata": {
        "id": "TPLtOEyrmUh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize both inputs and targets\n",
        "def tokenize_fn(batch):\n",
        "  # tokenize the input sequence first\n",
        "  # this populates input_ids. attention_mask, etc\n",
        "  tokenized_inputs = tokenizer(\n",
        "      batch['inputs'], truncation=True, is_split_into_words=True\n",
        "  )\n",
        "\n",
        "  labels_batch = batch['targets'] # original targets\n",
        "  aligned_labels_batch = []\n",
        "  for i, labels in enumerate(labels_batch):\n",
        "    word_ids = tokenized_inputs.word_ids(i)\n",
        "    aligned_labels_batch.append(align_targets(labels, word_ids))\n",
        "\n",
        "    # recall: the 'target' must be stored in key called 'labels'\n",
        "    tokenized_inputs['labels'] = aligned_labels_batch\n",
        "\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "3n7USEgfmvnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove XX from model inputs - they are neither inputs nor targets\n",
        "data[\"train\"].column_names"
      ],
      "metadata": {
        "id": "dqT_U_q-pIPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the tokenize function to each sample in our dataset, batching to make it more efficent and we remove the colums\n",
        "tokenized_datasets = data.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=data[\"train\"].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "nFJyp9fjpZtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the outputs\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "pE9IaiNbfARs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data collator\n",
        "from transformaers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "YDAW0_N7fEWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flattens a list of lists\n",
        "def flatten(lit_of_lists):\n",
        "  flattened = [val for sublist in list of lists for val in sublist]\n",
        "  return flattened"
      ],
      "metadata": {
        "id": "F-3ezNM2pLOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metrics function\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def compute_metrics(logits_and_labels):\n",
        "  logits, labels = logits_and_labels\n",
        "  preds = np.argmax(logits, axis= -1)\n",
        "\n",
        "  # remove -100 from labels and predictions\n",
        "  labels_jagged = [[t for t in label if t !=100] for label in labels]\n",
        "\n",
        "  # do the same for predictions whenever true label is -100\n",
        "  preds_jagged = [[p for p , t in zip(ps, ts) if t != -100] \\\n",
        "      for ps, ts in zip(preds, labels)\n",
        "  ]\n",
        "\n",
        "  # flatten labels nd preds\n",
        "  labels_flat = flatten(labels_jagged)\n",
        "  preds_flat = flatten(preds_jagged)\n",
        "\n",
        "  acc = accuracy_score(labels_flat, preds_flat)\n",
        "  f1 = f1_score(labels_flat, preds_flat, average='macro')\n",
        "\n",
        "  return {\n",
        "      'f1':f1,\n",
        "      'accuracy':acc,\n",
        "  }\n"
      ],
      "metadata": {
        "id": "FRUzr7-aqBdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our compute metrics function\n",
        "labels = [[-100, 0, 0, 1, 2, 1, -100]]\n",
        "logits = np.array([[\n",
        "    [0.8, 0.1, 0.1],\n",
        "    [0,8, 0.1, 0.1],\n",
        "    [0,8, 0.1, 0.1],\n",
        "    [0.1, 0.8, 0.1],\n",
        "    [0.1, 0.8, 0.1],\n",
        "    [0.1, 0.8, 0.1],\n",
        "    [0.1, 0.8, 0.1],\n",
        "]])\n",
        "compute_metrics((logits, labels))"
      ],
      "metadata": {
        "id": "qOWOL5K0sAFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load up our pre-trained model\n",
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ],
      "metadata": {
        "id": "Q9INKdqJs5RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training-arguments object\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"distilbert-finetuned-ner\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        ")"
      ],
      "metadata": {
        "id": "SE1v9omTte-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the trainer object an passing in what we've created above\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Triner(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenier=tokenizer\n",
        ")\n",
        "trainer.train() # Begins the fine-tuning process"
      ],
      "metadata": {
        "id": "TAmAp6d2uAUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our model\n",
        "trainer save_model('my_saved_model')"
      ],
      "metadata": {
        "id": "RqyHtxdaukYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load up our model as a pipeline object\n",
        "from ransformers import pipeline\n",
        "\n",
        "pipe = pipelie(\n",
        "    \"token-classification\",\n",
        "    model='my_saved_model',\n",
        "    device=0,\n",
        ")"
      ],
      "metadata": {
        "id": "rwgpweYfurci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the pipeline on a simple sentence\n",
        "s = \"Bill Gates was the CEO of Microsoft in Seattle, Washington.\"\n",
        "ner(s)"
      ],
      "metadata": {
        "id": "HvY_zjl1vBJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Bruce Wayne livs in Gotham City, he goes under the name Batman.\"\n",
        "ner(s)"
      ],
      "metadata": {
        "id": "8JKWGVsmvjr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Peter Parker is an American teenager .He lives in New York.What is he like ? He has got short straight brown hair, brown eyes, a long noseand a square face. He has got glasses. He is tall and slim but he isn't very strong !\"\n",
        "ner(s)"
      ],
      "metadata": {
        "id": "dbdTKpOdvmca"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}